{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isMaster: False\n",
      "possible background substraction model: dict_keys(['MOG2', 'KNN', 'CNT', 'LSBP', 'GSOC', 'PBAS'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    isMaster\n",
    "except NameError:\n",
    "    print('isMaster:', False)\n",
    "    from app.dotenv import base_dir, data_dir\n",
    "    from app.change_detect.background_baseline.background_subtractors import apply_bg_subtraction,save_background_model,methods\n",
    "    import os\n",
    "    print(f'possible background substraction model: {methods.keys()}')\n",
    "\n",
    "\n",
    "    video_fname = 'vtest.avi'\n",
    "    save_path = f'{data_dir}\\{video_fname.split(\".\")[0]}'\n",
    "    \n",
    "    # List all image files in both folders (can be the same folder)\n",
    "    folder_train_add = save_path\n",
    "    folder_evaluate_add = folder_train_add\n",
    "\n",
    "    \n",
    "background_models_add = f'{folder_train_add}/background_models'  # Folder to save background models\n",
    "os.makedirs(background_models_add, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "baseModel = 'MOG2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOG2 background substraction complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not  os.path.exists( os.path.join(background_models_add, f'background_model_{baseModel}.png')) :\n",
    "    print(f\"Processing with {baseModel}\")\n",
    "\n",
    "    # 1. Calculate the background model from images in image_files1\n",
    "    image_files_train = sorted([f\"{folder_train_add}/{f}\" for f in os.listdir(folder_train_add) if f.endswith(('.jpg', '.png'))]) # images for background modeling (training)\n",
    "    fgbg = apply_bg_subtraction(methods[baseModel], image_files_train)\n",
    "    print(f\"{baseModel} background model calculation complete.\")\n",
    "\n",
    "    # 2. Save the background model as an image - skip if method saving fails \n",
    "    if save_background_model(baseModel, fgbg, save_dir = background_models_add):\n",
    "        print(f\"{baseModel} background model saved.\")\n",
    "\n",
    "\n",
    "    # # 3. Apply the background model to images in image_files2 and display the foreground subtraction\n",
    "    # image_files_evaluate = sorted([f\"{folder_evaluate_add}/{f}\" for f in os.listdir(folder_evaluate_add) if f.endswith(('.jpg', '.png'))]) # images for foreground subtraction (testing)\n",
    "    # _ = apply_bg_subtraction(fgbg, image_files_evaluate, substract=True)\n",
    "    # # Clean up\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "print(f\"{baseModel} background substraction complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check diffrent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted_fgbg ={}\n",
    "# for method_name, fgbg in methods.items():\n",
    "#     print(f\"Processing with {method_name}\")\n",
    "\n",
    "#     # 1. Calculate the background model from images in image_files1\n",
    "#     ffgbg = apply_bg_subtraction(fgbg, image_files1,LEN=10)\n",
    "#     print(f\"{method_name} background model calculation complete.\")\n",
    "    \n",
    "#     # 2. Save the background model as an image - skip if method saving fails \n",
    "#     if save_background_model(method_name, fgbg, save_dir = background_models_add):\n",
    "#         print(f\"{method_name} background model saved.\")\n",
    "#         fitted_fgbg[method_name] = fgbg\n",
    "\n",
    "#     # 3. Apply the background model to images in image_files2 and display the foreground subtraction\n",
    "#     _ = apply_bg_subtraction(fgbg, image_files2, substract=True,LEN=10)\n",
    "#     # Clean up\n",
    "#     cv.destroyAllWindows()\n",
    "#     print(f\"{method_name} background substraction complete.\")\n",
    "    \n",
    "#     print('-'*50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f'models saved into images: {fitted_fgbg.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hybrid deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "# from segmentation_models_pytorch import Unet\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from app.dotenv import base_dir, data_dir\n",
    "\n",
    "# video_fname = 'vtest.avi'\n",
    "# video_path = f'{data_dir}\\{video_fname}'\n",
    "\n",
    "# # Load the pre-trained DeepLabV3 model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def get_model(model_name):\n",
    "#     \"\"\"Get the specified segmentation model\n",
    "#     Args: model_name (str): Name of the model to load ('DeepLabV3' or 'Unet')\n",
    "#     Returns: model: Segmentation model\n",
    "#              transform: Image transformation function\n",
    "    \n",
    "#     \"\"\"\n",
    "#     print(f\"Loading {model_name} model...\")\n",
    "#     if model_name == 'DeepLabV3':\n",
    "#         model = deeplabv3_resnet50(pretrained=True)\n",
    "#     elif model_name == 'Unet':\n",
    "#         model = Unet(\n",
    "#             encoder_name=\"resnet34\",  # You can change this to resnet50, etc.\n",
    "#             encoder_weights=\"imagenet\",  # Use ImageNet pre-trained weights\n",
    "#             in_channels=3,  # RGB image input\n",
    "#             classes=21,  # Number of output classes (adjust as needed for your segmentation task)\n",
    "#         )\n",
    "#     else:\n",
    "#         raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "    \n",
    "#     # Define transformation for input images\n",
    "#     # The mean and standard deviation (std) values are computed for the dataset on which the model is pretrained. \n",
    "#     # For ImageNet (for R, G, B channels), these values are:  \n",
    "#     meanImageNet=[0.485, 0.456, 0.406]\n",
    "#     stdImageNet=[0.229, 0.224, 0.225]\n",
    "#     transform = T.Compose([\n",
    "#         T.ToPILImage(),\n",
    "#         T.Resize((256, 256)),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=meanImageNet, std=stdImageNet)\n",
    "#     ])\n",
    "#     return model,transform\n",
    "    \n",
    "\n",
    "# model,transform = get_model('Unet')\n",
    "# model = model.eval().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def mog2_background_subtraction(frame, mog2):\n",
    "#     \"\"\"Apply MOG2 Background Subtraction.\"\"\"\n",
    "#     fg_mask = mog2.apply(frame)\n",
    "#     return fg_mask\n",
    "\n",
    "# def deep_learning_refinement(frame, model):\n",
    "#     \"\"\"Refine MOG2 mask with deep learning-based segmentation.\"\"\"\n",
    "#     input_tensor = transform(frame).unsqueeze(0).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input_tensor)\n",
    "#     output = output[0] if type(output) == torch.Tensor else output['out'][0] # Output segmentation map: torch.Size([21, 256, 256])\n",
    "#     segmentation_map = output.argmax(0).byte().cpu().numpy()\n",
    "#     return segmentation_map\n",
    "\n",
    "# def deep_learning_refinement(frame, model):\n",
    "#     \"\"\"Refine MOG2 mask with deep learning-based segmentation.\"\"\"\n",
    "    \n",
    "#     # Transform the frame (input image) into a format suitable for the model.\n",
    "#     # The frame is converted into a tensor and resized to the expected input size.\n",
    "#     # The transformation also normalizes the image using pre-trained model's mean and std.\n",
    "#     input_tensor = transform(frame).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Use the model to predict the segmentation map for the input image.\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input_tensor) \n",
    "#     # Output: tensor of shape [21, 256, 256] where 21 corresponds to the number of classes (including background).\n",
    "#     output = output[0] if type(output) == torch.Tensor else output['out'][0]  \n",
    "\n",
    "#     # Apply the argmax operation across the 'channels' (axis 0) to get the most likely class for each pixel.\n",
    "#     # The model output for each pixel is a vector of 21 class probabilities, and we select the class with the highest probability.\n",
    "#     segmentation_map = output.argmax(0).byte().cpu().numpy()\n",
    "    \n",
    "#     # Explanation of argmax:\n",
    "#     # The output tensor has shape [21, 256, 256], with each \"slice\" (along axis 0) representing the \n",
    "#     # predicted probability map for one of the 21 classes.\n",
    "#     # `argmax(0)` selects the index of the class with the highest probability for each pixel.\n",
    "#     # The result is a 2D map (256, 256) with pixel values representing class indices (0 to 20).\n",
    "\n",
    "#     return segmentation_map\n",
    "\n",
    "\n",
    "\n",
    "# # def combine_masks(mog2_mask, dl_mask):\n",
    "# #     \"\"\"Combine MOG2 and Deep Learning masks.\"\"\"\n",
    "# #     combined = cv2.bitwise_and(mog2_mask, mog2_mask, mask=dl_mask)\n",
    "# #     return combined\n",
    "\n",
    "# def combine_masks(mog2_mask, dl_mask):\n",
    "#     \"\"\"Combine MOG2 and Deep Learning masks.\"\"\"\n",
    "#     # Resize MOG2 mask to match the shape of the DeepLabV3 mask\n",
    "#     resized_mog2_mask = cv2.resize(mog2_mask, (dl_mask.shape[1], dl_mask.shape[0]))\n",
    "\n",
    "#     # Combine masks (bitwise AND)\n",
    "#     combined = cv2.bitwise_and(resized_mog2_mask, resized_mog2_mask, mask=dl_mask)\n",
    "#     return combined\n",
    "\n",
    "# # Initialize MOG2\n",
    "# mog2 = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# # Video Capture\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # while True:\n",
    "# #     ret, frame = cap.read()\n",
    "# #     if not ret:\n",
    "# #         break\n",
    "\n",
    "# #     # Step 1: MOG2 Background Subtraction\n",
    "    \n",
    "# #     fg_mask = mog2.apply(frame)\n",
    "\n",
    "# #     # Step 2: Deep Learning-Based Refinement\n",
    "# #     dl_mask = deep_learning_refinement(frame, model)\n",
    "\n",
    "# #     # Step 3: Combine Masks\n",
    "# #     combined_mask = combine_masks(fg_mask, dl_mask)\n",
    "\n",
    "# #     # Visualization\n",
    "# #     cv2.imshow(\"Original Frame\", frame)\n",
    "# #     cv2.imshow(\"MOG2 Mask\", fg_mask)\n",
    "# #     cv2.imshow(\"Deep Learning Mask\", dl_mask * 255)\n",
    "# #     cv2.imshow(\"Combined Mask\", combined_mask)\n",
    "\n",
    "# #     if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "# #         break\n",
    "\n",
    "# # cap.release()\n",
    "# # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Step 1: MOG2 Background Subtraction\n",
    "#     fg_mask = mog2.apply(frame)\n",
    "\n",
    "#     # Step 2: Deep Learning-Based Refinement\n",
    "#     dl_mask = deep_learning_refinement(frame, model)\n",
    "\n",
    "#     # Step 3: Combine Masks\n",
    "#     combined_mask = combine_masks(fg_mask, dl_mask)\n",
    "\n",
    "#     # Resize masks to match the original frame size for display\n",
    "#     fg_mask_resized = cv2.resize(fg_mask, (frame.shape[1], frame.shape[0]))\n",
    "#     dl_mask_resized = cv2.resize(dl_mask * 255, (frame.shape[1], frame.shape[0]))  # Scaling for display\n",
    "#     combined_mask_resized = cv2.resize(combined_mask, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "#     # Create a figure with 1 row and 4 columns (for original frame and three masks)\n",
    "#     fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "#     # Display each image in its own subplot\n",
    "#     axs[0].imshow(frame)\n",
    "#     axs[0].set_title(\"Original Frame\")\n",
    "#     axs[0].axis(\"off\")  # Hide axes\n",
    "\n",
    "#     axs[1].imshow(fg_mask_resized, cmap='gray')\n",
    "#     axs[1].set_title(\"MOG2 Mask\")\n",
    "#     axs[1].axis(\"off\")  # Hide axes\n",
    "\n",
    "#     axs[2].imshow(dl_mask_resized, cmap='gray')\n",
    "#     axs[2].set_title(\"Deep Learning Mask\")\n",
    "#     axs[2].axis(\"off\")  # Hide axes\n",
    "\n",
    "#     axs[3].imshow(combined_mask_resized, cmap='gray')\n",
    "#     axs[3].set_title(\"Combined Mask\")\n",
    "#     axs[3].axis(\"off\")  # Hide axes\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
